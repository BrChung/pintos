			+--------------------+
			|    CompSci 143A    |
			| PROJECT 1: THREADS |
			|   DESIGN DOCUMENT  |
			+--------------------+
				   
---- GROUP ----

>> Fill in the names and email addresses of your group members.

Taiga Miyano <amiyano@uci.edu>
Aditya Kuppili <akuppili@uci.edu>
Brian Chung <bchung4@uci.edu>

---- PRELIMINARIES ----

>> If you have any preliminary comments on your submission or notes for the
>> TAs, please give them here.

>> Please cite any offline or online sources you consulted while
>> preparing your submission, other than the Pintos documentation, course
>> text, lecture notes, and course staff.

			     ALARM CLOCK
			     ===========

---- DATA STRUCTURES ----

>> A1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.
In thread.h:
struct thread {
    ...
    int64_t wakeup_timer;
}
The attribute in the thread struct stores the earliest wake up time for the sleeping thread.


In timer.h:
static struct list threads_slept;

Above is a list that stores all of the threads that are currently sleeping. This list is managed by the timer and is ordered by wake up times.

---- ALGORITHMS ----

>> A2: Briefly describe what happens in a call to timer_sleep(),
>> including the effects of the timer interrupt handler.

In timer_sleep(), we calculate the earliest time in ticks when the thread should be woken up and add the current thread to the global list of sleeping threads (threads_slept). We then block the thread.

>> A3: What steps are taken to minimize the amount of time spent in
>> the timer interrupt handler?

The global list/queue of sleeping threads is ordered, where the thread with the earliest wake up time is at the front. Therefore the interrupt handler needs to only check the front of the list to see if there exists a thread where the wake up time has passed. If there are threads that need waking up, the interrupt handler will traverse the list until it finds the first thread that is still sleeping. If there are no threads that need waking up after checking the first thread in threads_slept, the interrupt handler can move on.

---- SYNCHRONIZATION ----

>> A4: How are race conditions avoided when multiple threads call
>> timer_sleep() simultaneously?

To prevent race conditions of multiple threads calling timer_sleep(), we first disabled interrupts whenever we traverse or modify the data structures listed in part A1. This ensures that all reads and writes are atomic. We also added a mutex lock to timer_sleep() in particular to prevent a race condition among multiple threads.

>> A5: How are race conditions avoided when a timer interrupt occurs
>> during a call to timer_sleep()?

Interrupts are disabled during the critical sections of timer_sleep. Therefore, a timer interrupt cannot occur. There will be no race conditions between a timer interrupt and timer_sleep.

---- RATIONALE ----

>> A6: Why did you choose this design?  In what ways is it superior to
>> another design you considered?

We initially considered storing the time remaining on all sleeping threads; however, that would require us to decrement the remaining wait times on every tick and traverse all threads_slept. By storing the wake up time and keeping the slept threads sorted, we are able to efficiently wake up threads during interrupts without the extra computation of tracking time remaining.

			 PRIORITY SCHEDULING
			 ===================

---- DATA STRUCTURES ----

>> B1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

In thread struct:
    int d_num: stores the number of donation locks in a given thread
    int priority_list[9]: list that stores the priorities
    int p_size: size the of the priority list
    struct lock *wait: the lock for when a blocked thread has to wait

In lock struct:
    bool donate: a boolean that stores whether the lock is donated to a thread or not


>> B2: Explain the data structure used to track priority donation.
>> Use ASCII art to diagram a nested donation.  (Alternately, submit a
>> .png file.)

We create a list in the thread struct to keep track of the donated priorities. We also have a lock holder that inherits its priority whenever a lock is requested. 

For image see ./B2.png

---- ALGORITHMS ----

>> B3: How do you ensure that the highest priority thread waiting for
>> a lock, semaphore, or condition variable wakes up first?

To ensure that the highest priority thread wakes up first, we modify the insertion of the threads into the ready list. We use the compare_priority function as a parameter to the list_ordered_function that tells the function how to compare priorities. We also have a compare_semaphore function that does the same thing except with semaphores also used as a parameter inside the list_ordered_function. If a donation operation occurs, we re-sort the list. This will allow us to always maintain the order of the ready_list. 

>> B4: Describe the sequence of events when a call to lock_acquire()
>> causes a priority donation.  How is nested donation handled?

If a thread starts waiting on a lock, we check whether its priority is higher than the priority of the lock’s holder. If it is, we then use our priority_list to facilitate the donations of priorities. To support nested donation, we keep looping until we reach a process that is not waiting on a lock. We then sema_down afterwards. 

>> B5: Describe the sequence of events when lock_release() is called
>> on a lock that a higher-priority thread is waiting for.

We remove the lock from the list of locks acquired by the current thread. We then set the priority to the highest priority of the threads waiting on the lock. We then are able to release the lock. 

---- SYNCHRONIZATION ----

>> B6: Describe a potential race in thread_set_priority() and explain
>> how your implementation avoids it.  Can you use a lock to avoid
>> this race?

One potential race condition would be if a current thread is interrupted by another thread trying to set the current thread’s priority via donation while the current thread is also trying to set its own priority calling the thread_set_priority() function. We can use a lock to avoid this by forcing the threads to first acquire a lock to the thread priority before setting the priorities. 

---- RATIONALE ----

>> B7: Why did you choose this design?  In what ways is it superior to
>> another design you considered?

The design Pintos uses is FIFO scheduling. The goal of this part was to implement priority scheduling. To accomplish this, we sorted the ready list by the thread priority and also sorted the wait list for synchronization. We also implemented preemption along with finding the preemption point when the thread is put into the ready list. We’ve considered keeping the ready list unsorted as selecting the thread with the highest priority on retrieval would be linear, however we’ve decided to sort whenever a thread is inserted or updated in the list for simplicity.

			  ADVANCED SCHEDULER
			  ==================

---- DATA STRUCTURES ----

>> C1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

In thread.h:
struct thread {
    ...
    int nice;
    Float recent_cpu;
}
Within the thread definition, nice stores the current nice value of the thread. The ‘recent_cpu’ attribute stores the approximate CPU the thread has been using.

In thread.c
static Float load_avg = 0;

The load_avg is global variable that stores the current load average

---- ALGORITHMS ----

>> C2: Suppose threads A, B, and C have nice values 0, 1, and 2.  Each
>> has a recent_cpu value of 0.  Fill in the table below showing the
>> scheduling decision and the priority and recent_cpu values for each
>> thread after each given number of timer ticks:

timer  recent_cpu      priority        thread
ticks   A   B   C     A      B     C   to run
-----  --  --   --   ---   ---   ---   ------
0      0   0    0   63.00 61.00 59.00   A
4      4   0    0   62.00 61.00 59.00   A
8      8   0    0   61.00 61.00 59.00   A
12     12  0    0   60.00 61.00 59.00   B
16     12  4    0   60.00 60.00 59.00   B
20     12  8    0   60.00 59.00 59.00   A
24     16  8    0   59.00 59.00 59.00   A
28     20  8    0   58.00 59.00 59.00   C
32     20  8    4   58.00 59.00 58.00   B
36     20  12   4   58.00 58.00 58.00   B

>> C3: Did any ambiguities in the scheduler specification make values
>> in the table uncertain?  If so, what rule did you use to resolve
>> them?  Does this match the behavior of your scheduler?

Yes, the ambiguity lies in whether or not the recent_cpu needs to update before or after updating the priorities. In our table we decided to update the recent_cpu before updating the priority.

>> C4: How is the way you divided the cost of scheduling between code
>> inside and outside interrupt context likely to affect performance?

We divided the cost of scheduling by performing the multilevel feedback queue scheduling in the interrupt context. Outside the interrupt context, our logic decides the next thread to run. Since this happens outside the interrupt and the number of operations is comparatively small, dividing the cost of scheduling this way improves the performance. This prevents the thread from allocating extra time for the scheduler to decide which thread to run next.

---- RATIONALE ----

>> C5: Briefly critique your design, pointing out advantages and
>> disadvantages in your design choices.  If you were to have extra
>> time to work on this part of the project, how might you choose to
>> refine or improve your design?

We’ve kept our design similar to the 4.4BSD Scheduler; we regularly recalculate the recent_cpu for all threads and load_avg of running threads. Every four ticks, we recalculate the thread priorities - this requires traversing the entire list of threads. Our implementation is simple, however, given more time to refine our design, we may have opted to store an array of a list of ready threads. This may assist in calculations as we would know the count of threads in the MLFQS, changing our linear load average calculation to constant time. We can also improve performance by tracking the highest priority of any given thread - this may allow us to find the next thread during scheduling slightly faster.

>> C6: The assignment explains arithmetic for fixed-point math in
>> detail, but it leaves it open to you to implement it.  Why did you
>> decide to implement it the way you did?  If you created an
>> abstraction layer for fixed-point math, that is, an abstract data
>> type and/or a set of functions or macros to manipulate fixed-point
>> numbers, why did you do so?  If not, why not?

We implemented the fixed-point math by using an abstraction layer for the fixed point math. We defined it in fixed-point.h and called it Float in the threads folder. We did it this way so our code was more structured and organized and it made it easier to understand how specific arithmetic changes would impact the scheduler.

			   SURVEY QUESTIONS
			   ================

Answering these questions is optional, but it will help us improve the
course in future quarters.  Feel free to tell us anything you
want--these questions are just to spur your thoughts.  You may also
choose to respond anonymously in the course evaluations at the end of
the quarter.

>> In your opinion, was this assignment, or any one of the three problems
>> in it, too easy or too hard?  Did it take too long or too little time?

>> Did you find that working on a particular part of the assignment gave
>> you greater insight into some aspect of OS design?

>> Is there some particular fact or hint we should give students in
>> future quarters to help them solve the problems?  Conversely, did you
>> find any of our guidance to be misleading?

>> Do you have any suggestions for the TAs to more effectively assist
>> students, either for future quarters or the remaining projects?

>> Any other comments?